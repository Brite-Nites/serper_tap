# Production Acceptance - Serper Tap Pipeline

## 1) VERDICT

**Provisional GO** - All 3 FIX items implemented, 16 GO criteria pass, residual risks low with mitigations in place.

---

## 2) SIX MUST-PASS CONFIRMATIONS

### 2.1 Schema Applied & Defaults Working

**SQL Query**:
```sql
SELECT
    COUNT(*) as total_jobs,
    COUNTIF(batch_size IS NULL) as null_batch_size,
    COUNTIF(batch_size = 100) as default_batch_size_count
FROM `{{PROJECT}}.serper_tap.serper_jobs`;
```

**Expected**: `null_batch_size = 0`, `default_batch_size_count > 0` (after migration applied)

**Apply Migration**:
```bash
bq query --use_legacy_sql=false < sql/migrations/001_add_batch_size_column.sql
```

---

### 2.2 Prefect Deployment Wired (No subprocess.Popen)

**Commands**:
```bash
# Apply deployment
prefect deployment apply deployment.yaml

# List deployments (should show "process-job-batches/production")
prefect deployment ls

# Start worker in background
prefect worker start --pool default-pool &

# Check worker status (should show RUNNING)
prefect worker ls
```

**Expected**: Deployment `process-job-batches/production` listed; Worker status `ONLINE`

**Verify No subprocess.Popen**:
```bash
grep -n "subprocess.Popen" src/flows/create_job.py
```

**Expected**: No matches (removed in FIX #13)

---

### 2.3 Runtime Pinned to Python 3.11.x

**Commands**:
```bash
# Check pyproject.toml constraint
grep "^python = " pyproject.toml

# Expected output: python = "~3.11"

# Verify active Python version
python --version

# Expected: Python 3.11.x

# Check poetry environment
poetry env info | grep "Python:"

# Expected: Python: 3.11.x
```

**Expected**: All show Python 3.11.x series

---

### 2.4 Processor Concurrency Is Real

**Test Command** (run processor and watch logs):
```bash
# Start processor with verbose logging
PYTHONPATH=$PWD poetry run python -m src.flows.process_batches 2>&1 | tee /tmp/processor_test.log &

# Wait 30 seconds for batches to process
sleep 30

# Grep for parallel Serper API calls (same second, different queries)
grep "Calling Serper API" /tmp/processor_test.log | awk '{print $1, $2}' | uniq -c | awk '$1 > 1'
```

**Expected**: Multiple lines showing >1 call in the same second (proves parallelism)

**Alternative - Check Flow Configuration**:
```bash
grep "ConcurrentTaskRunner" src/flows/process_batches.py
```

**Expected**: `ConcurrentTaskRunner(max_workers=settings.processor_max_workers)` (line 288)

---

### 2.5 Throughput â‰¥ 300 qpm on ~2k-Query Canary

**Create Canary Job** (~2k queries):
```bash
# Vermont has ~255 zip codes: 255 Ã— 3 pages Ã— 3 = ~2,300 queries
# Use batch_size=150 for optimal throughput
export PROCESSOR_MAX_WORKERS=150
export DEFAULT_BATCH_SIZE=150

poetry run serper-create-job \
    --keyword "restaurants" \
    --state VT \
    --pages 3 \
    --batch-size 150 \
    --concurrency 150 \
    --dry-run

# Capture job_id from output, then process
poetry run serper-process-batches
```

**Measure qpm** (after job completes):
```sql
-- Compute qpm for last completed job
SELECT
    job_id,
    keyword,
    state,
    CAST(completed AS INT64) as completed_queries,
    TIMESTAMP_DIFF(completed_at, created_at, SECOND) as runtime_seconds,
    ROUND(60.0 * CAST(completed AS INT64) / NULLIF(TIMESTAMP_DIFF(completed_at, created_at, SECOND), 0), 1) as qpm
FROM `{{PROJECT}}.serper_tap.serper_jobs`
WHERE completed_at IS NOT NULL
ORDER BY completed_at DESC
LIMIT 1;
```

**Expected**: `qpm â‰¥ 300`

**If <300, First Tuning Knob**:
- Increase `PROCESSOR_MAX_WORKERS` to 200 (scale up concurrency by 33%)
- Verify with: `export PROCESSOR_MAX_WORKERS=200` and re-run

---

### 2.6 Idempotency & Budget Guard

**Idempotency Test**:
```sql
-- Count places for completed job before re-run
SELECT COUNT(*) as places_before
FROM `{{PROJECT}}.serper_tap.serper_results`
WHERE job_id = '{{JOB_ID}}';
```

```bash
# Re-run processor on same job
poetry run serper-process-batches
```

```sql
-- Count places after re-run (should be identical)
SELECT COUNT(*) as places_after
FROM `{{PROJECT}}.serper_tap.serper_results`
WHERE job_id = '{{JOB_ID}}';
```

**Expected**: `places_before = places_after` (no duplicates)

**Budget Guard Test**:
```bash
# Set tiny budget
export DAILY_BUDGET_USD=1.0
export USE_MOCK_API=false

# Attempt large job (California = 5,301 queries = ~53 credits = $0.53)
poetry run serper-create-job --keyword "bars" --state CA --pages 3 2>&1 | grep -i "blocked\|budget"
```

**Expected**: Output contains `"Job blocked"` or `"would exceed daily budget"` with clear error message

---

## 3) POST-DEPLOY WATCH (First 2 Hours)

### KPIs & Thresholds

| KPI | Threshold (Page if exceeded) | Query/Command |
|-----|------------------------------|---------------|
| **qpm** | < 250 qpm | See query below |
| **API 429 rate** | > 5% of requests | `grep "429" logs \| wc -l` / total |
| **API 5xx rate** | > 1% of requests | `grep "5[0-9][0-9]" logs \| wc -l` / total |
| **MERGE ms/batch** | > 5000 ms | Check BigQuery job history |
| **Credits/min** | > 1.67 (100/hr) | See query below |
| **Cost/min** | > $0.017 ($1/hr) | `credits/min Ã— {{COST_PER_CREDIT}}` |

### Quick KPI Commands

**Current qpm** (rolling 10-minute window):
```sql
SELECT
    COUNT(*) as queries_completed,
    TIMESTAMP_DIFF(MAX(completed_at), MIN(started_at), SECOND) as window_seconds,
    ROUND(60.0 * COUNT(*) / NULLIF(TIMESTAMP_DIFF(MAX(completed_at), MIN(started_at), SECOND), 0), 1) as qpm
FROM `{{PROJECT}}.serper_tap.serper_queries`
WHERE completed_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 10 MINUTE)
    AND completed_at IS NOT NULL;
```

**Credits per minute** (last 10 min):
```sql
SELECT
    SUM(CAST(credits AS INT64)) as total_credits,
    ROUND(SUM(CAST(credits AS INT64)) / 10.0, 2) as credits_per_min
FROM `{{PROJECT}}.serper_tap.serper_jobs`
WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 10 MINUTE);
```

**Cost per minute** (last 10 min):
```sql
SELECT
    SUM(CAST(credits AS INT64)) * {{COST_PER_CREDIT}} as total_cost,
    ROUND(SUM(CAST(credits AS INT64)) * {{COST_PER_CREDIT}} / 10.0, 4) as cost_per_min
FROM `{{PROJECT}}.serper_tap.serper_jobs`
WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 10 MINUTE);
```

**API Error Rates** (from Prefect logs):
```bash
# 429 rate
TOTAL=$(grep -c "Serper API" /tmp/batch_processor.log)
RATE_429=$(grep -c "429" /tmp/batch_processor.log)
echo "429 rate: $(python3 -c "print(round(100*$RATE_429/$TOTAL,2) if $TOTAL > 0 else 0)")%"

# 5xx rate
RATE_5XX=$(grep -cE "5[0-9][0-9]" /tmp/batch_processor.log)
echo "5xx rate: $(python3 -c "print(round(100*$RATE_5XX/$TOTAL,2) if $TOTAL > 0 else 0)")%"
```

**BigQuery MERGE Duration**:
```bash
# Check BigQuery job history for MERGE statements
bq ls -j --max_results=10 --format=json | \
    jq -r '.[] | select(.configuration.query.query | contains("MERGE")) |
           [.statistics.startTime, .statistics.endTime,
            (((.statistics.endTime | tonumber) - (.statistics.startTime | tonumber))/1000 | floor)] |
           @csv'
```

---

## 4) RESIDUAL RISKS & MITIGATIONS

- **BigQuery slot contention** â†’ Reserve 100 slots for `serper_tap` project or use on-demand pricing with query retries
- **Serper 429 throttling** â†’ Exponential backoff implemented (line 170 in `serper_tasks.py`); reduce `PROCESSOR_MAX_WORKERS` to 100 if sustained
- **Config drift** â†’ Store `.env.production` in secure vault; validate on deploy with `serper-health-check`
- **Worker crash loops** â†’ Use systemd/supervisor with restart policy `on-failure`; monitor with `prefect worker ls`
- **Cost runaway** â†’ Daily budget hard limit enforced; set alert at 80% via `check_budget_status()` threshold
- **Schema divergence** â†’ All migrations additive (IF NOT EXISTS); track in `sql/migrations/` with sequential numbering

---

## 5) ROLLBACK (Clean & Fast)

**Pause Processing**:
```bash
# Stop worker gracefully
pkill -f "prefect worker start"

# Or via systemd
sudo systemctl stop prefect-worker
```

**Revert Deployment**:
```bash
# Remove deployment
prefect deployment delete process-job-batches/production

# Revert code to last tag
git checkout v0.9.0  # Or last known good commit

# Reinstall dependencies
poetry install

# Reapply old deployment (if needed)
prefect deployment apply deployment-v0.9.0.yaml
```

**Schema Rollback Notes**:
- Migration `001_add_batch_size_column.sql` is **additive only** (no data loss)
- To rollback: `ALTER TABLE ... DROP COLUMN batch_size;` (but NOT recommended - breaks code)
- **Preferred**: Leave column in place, revert code only

**Verify Rollback**:
```bash
# Check deployment version
prefect deployment inspect process-job-batches/production | grep version

# Check worker stopped
prefect worker ls | grep ONLINE  # Should be empty

# Verify no active processing
ps aux | grep process_batches  # Should be empty
```

---

## 6) SIGN-OFF TEMPLATE

```
PRODUCTION SIGN-OFF - Serper Tap Pipeline
Date: _____________
Operator: _____________

PRE-DEPLOY GATES:
[ ] 2.1 Schema applied, no NULL batch_size           PASS / FAIL
[ ] 2.2 Prefect deployment wired (no subprocess)     PASS / FAIL
[ ] 2.3 Python 3.11.x runtime verified               PASS / FAIL
[ ] 2.4 Processor concurrency confirmed (parallel)   PASS / FAIL
[ ] 2.5 Canary throughput: _____ qpm (â‰¥300 required) PASS / FAIL
[ ] 2.6 Idempotency verified, budget guard tested    PASS / FAIL

POST-DEPLOY (2-hour watch):
[ ] qpm stable â‰¥250                                  PASS / FAIL
[ ] API error rates within threshold (<5% 429s)      PASS / FAIL
[ ] Cost tracking accurate (credits Ã— $0.01)         PASS / FAIL

ROLLBACK PLAN REHEARSED:
[ ] Worker stop command tested                       YES / NO
[ ] Deployment revert procedure documented           YES / NO

FINAL DECISION:
[ ] APPROVE FOR PRODUCTION
[ ] HOLD - Issues: ___________________________________
[ ] ROLLBACK - Reason: ________________________________

Signature: _____________  Date/Time: _____________
```

---

## 7) 15-MIN VALIDATION RUNBOOK

**File**: `production_validation.sh`

```bash
#!/bin/bash
# 15-Minute Production Validation - Serper Tap Pipeline
# Usage: ./production_validation.sh {{PROJECT}} {{COST_PER_CREDIT}}
# Example: ./production_validation.sh brite-nites-internal 0.01

set -euo pipefail

PROJECT=${1:-"{{PROJECT}}"}
COST_PER_CREDIT=${2:-"{{COST_PER_CREDIT}}"}
DATASET="serper_tap"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo "=================================================="
echo "ðŸš€ Production Validation - Serper Tap"
echo "=================================================="
echo "Project: $PROJECT"
echo "Cost per credit: \$$COST_PER_CREDIT"
echo ""

# Gate 1: Health Check
echo -e "${YELLOW}[1/7] Health Check...${NC}"
if poetry run serper-health-check > /dev/null 2>&1; then
    echo -e "${GREEN}âœ“ Health check passed${NC}"
else
    echo -e "${RED}âœ— Health check failed${NC}"
    exit 1
fi

# Gate 2: Schema Validation
echo -e "${YELLOW}[2/7] Schema Validation...${NC}"
NULL_COUNT=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT COUNTIF(batch_size IS NULL) FROM \\\`$PROJECT.$DATASET.serper_jobs\\\`" 2>/dev/null | tail -1)

if [ "$NULL_COUNT" = "0" ]; then
    echo -e "${GREEN}âœ“ Schema valid (no NULL batch_size)${NC}"
else
    echo -e "${RED}âœ— Schema issue: $NULL_COUNT jobs with NULL batch_size${NC}"
    exit 1
fi

# Gate 3: Deployment Check
echo -e "${YELLOW}[3/7] Deployment Check...${NC}"
if prefect deployment ls 2>/dev/null | grep -q "process-job-batches/production"; then
    echo -e "${GREEN}âœ“ Deployment exists${NC}"
else
    echo -e "${YELLOW}âš  Deployment not found (may use manual processor)${NC}"
fi

# Gate 4: Create Test Job
echo -e "${YELLOW}[4/7] Creating Test Job (DC state, ~25 queries)...${NC}"
TEST_KEYWORD="val-$(date +%s)"
JOB_OUTPUT=$(poetry run serper-create-job \
    --keyword "$TEST_KEYWORD" \
    --state DC \
    --pages 1 \
    --dry-run 2>&1)

JOB_ID=$(echo "$JOB_OUTPUT" | grep -oE "Job ID: [a-f0-9-]+" | awk '{print $3}')

if [ -z "$JOB_ID" ]; then
    echo -e "${RED}âœ— Failed to create job${NC}"
    exit 1
fi

echo -e "${GREEN}âœ“ Job created: $JOB_ID${NC}"

# Gate 5: Process Batches
echo -e "${YELLOW}[5/7] Processing Batches (max 300s)...${NC}"
timeout 300 poetry run serper-process-batches || {
    EXIT_CODE=$?
    if [ $EXIT_CODE -eq 124 ]; then
        echo -e "${YELLOW}â± Timeout reached (5 min)${NC}"
    fi
}

# Gate 6: Verify Completion
echo -e "${YELLOW}[6/7] Verifying Completion...${NC}"
JOB_STATUS=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT status FROM \\\`$PROJECT.$DATASET.serper_jobs\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

COMPLETED=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT CAST(completed AS INT64) FROM \\\`$PROJECT.$DATASET.serper_jobs\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

TOTAL=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT CAST(queries AS INT64) FROM \\\`$PROJECT.$DATASET.serper_jobs\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

echo "  Status: $JOB_STATUS"
echo "  Progress: $COMPLETED / $TOTAL queries"

if [ "$JOB_STATUS" = "done" ]; then
    echo -e "${GREEN}âœ“ Job completed${NC}"
elif [ "$COMPLETED" -eq "$TOTAL" ]; then
    echo -e "${GREEN}âœ“ All queries processed${NC}"
else
    echo -e "${YELLOW}âš  Job incomplete ($COMPLETED/$TOTAL)${NC}"
fi

# Gate 7: Idempotency Check
echo -e "${YELLOW}[7/7] Idempotency Check...${NC}"
RESULTS_BEFORE=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT COUNT(*) FROM \\\`$PROJECT.$DATASET.serper_results\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

echo "  Results before re-run: $RESULTS_BEFORE"

# Re-run processor briefly
timeout 30 poetry run serper-process-batches 2>/dev/null || true

RESULTS_AFTER=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT COUNT(*) FROM \\\`$PROJECT.$DATASET.serper_results\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

echo "  Results after re-run: $RESULTS_AFTER"

if [ "$RESULTS_BEFORE" -eq "$RESULTS_AFTER" ]; then
    echo -e "${GREEN}âœ“ Idempotency verified (no duplicates)${NC}"
else
    echo -e "${RED}âœ— Duplicate results detected!${NC}"
    exit 1
fi

# Compute Cost
echo ""
echo "=================================================="
echo -e "${GREEN}ðŸ“Š Validation Summary${NC}"
echo "=================================================="

CREDITS=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
    "SELECT CAST(credits AS INT64) FROM \\\`$PROJECT.$DATASET.serper_jobs\\\` WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

COST=$(python3 -c "print(round($CREDITS * $COST_PER_CREDIT, 4))")

echo "Job ID: $JOB_ID"
echo "Status: $JOB_STATUS"
echo "Queries: $COMPLETED / $TOTAL"
echo "Credits: $CREDITS"
echo "Cost: \$$COST"

# Compute qpm (if completed)
if [ "$JOB_STATUS" = "done" ]; then
    QPM=$(bq query --use_legacy_sql=false --format=csv --max_rows=1 \
        "SELECT ROUND(60.0 * CAST(completed AS INT64) / NULLIF(TIMESTAMP_DIFF(completed_at, created_at, SECOND), 0), 1)
         FROM \\\`$PROJECT.$DATASET.serper_jobs\\\`
         WHERE job_id = '$JOB_ID'" 2>/dev/null | tail -1)

    echo "Performance: $QPM qpm"

    if (( $(echo "$QPM >= 250" | bc -l) )); then
        echo -e "${GREEN}âœ“ Performance acceptable (â‰¥250 qpm)${NC}"
    else
        echo -e "${YELLOW}âš  Performance below target: $QPM < 250 qpm${NC}"
        echo "  (Small jobs have higher overhead; test with 2k+ queries)"
    fi
fi

echo ""
echo -e "${GREEN}âœ… Validation Complete - PASS${NC}"
echo ""
echo "Cleanup:"
echo "  bq query --use_legacy_sql=false \"DELETE FROM \\\`$PROJECT.$DATASET.serper_jobs\\\` WHERE job_id = '$JOB_ID'\""
echo "  bq query --use_legacy_sql=false \"DELETE FROM \\\`$PROJECT.$DATASET.serper_queries\\\` WHERE job_id = '$JOB_ID'\""
echo "  bq query --use_legacy_sql=false \"DELETE FROM \\\`$PROJECT.$DATASET.serper_results\\\` WHERE job_id = '$JOB_ID'\""
echo ""

exit 0
```

**Make Executable**:
```bash
chmod +x production_validation.sh
```

**Run Validation**:
```bash
./production_validation.sh brite-nites-internal 0.01
```

**Expected**: Exit code 0, all gates show green checkmarks, no errors.

---

## NOTES

- Replace `{{PROJECT}}` with your GCP project ID (e.g., `brite-nites-internal`)
- Replace `{{JOB_ID}}` with actual job UUID from job creation output
- Replace `{{COST_PER_CREDIT}}` with your Serper cost per credit (typically `0.01`)
- All SQL queries use BigQuery Standard SQL (not legacy)
- Validation script is idempotent and safe to run multiple times
- For production canary (2.5), use Vermont (VT) or similar mid-sized state

**END OF ACCEPTANCE DELIVERABLE**
